{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proposal_SF_PongNFS_0_copy_ipynb_のコピー.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hv-Dqojqm7Nh"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3Xq_P3TGwTx",
        "colab_type": "text"
      },
      "source": [
        "# DQN implementation with PyTorch using PongNoFrameskip-v4 benchmark.\n",
        "\n",
        "In this notebook, we implement Deep Q-Network (DQN), one of the rainforcement learning algorithm, using `PyTorch`.  \n",
        "This code refers to [jmichaux/dqn-pytorch](https://github.com/jmichaux/dqn-pytorch)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QfzACsx863h",
        "colab_type": "text"
      },
      "source": [
        "In this code, we propose the new method of improve the escape from local-minimum of rainforcement learning. Our method takeing multi-agent approach. In addition, we also define the agent durabiity inspired by Evolutionary computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKSJOvllG1Iu",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FytgoxIUPaIp",
        "colab_type": "code",
        "outputId": "bc8c2e3a-2339-476a-f3b0-e337e503ee1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt-get install -y cmake zlib1g-dev libjpeg-dev xvfb ffmpeg xorg-dev python-opengl libboost-all-dev libsdl2-dev swig freeglut3-dev\n",
        "!pip install -U gym imageio PILLOW pyvirtualdisplay 'gym[atari]' 'pyglet==1.3.2' pyopengl scipy JSAnimation opencv-python pillow h5py pyyaml hyperdash pyvirtualdisplay hyperdash\n",
        "!apt-get install xvfb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libjpeg-dev is already the newest version (8c-2ubuntu8).\n",
            "libjpeg-dev set to manually installed.\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "zlib1g-dev set to manually installed.\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "freeglut3-dev set to manually installed.\n",
            "libboost-all-dev is already the newest version (1.65.1.0ubuntu1).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "ffmpeg is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n",
            "The following additional packages will be installed:\n",
            "  gir1.2-ibus-1.0 libcapnp-0.6.1 libdbus-1-dev libdmx-dev libdmx1\n",
            "  libfontenc-dev libfs-dev libfs6 libibus-1.0-5 libibus-1.0-dev\n",
            "  libmirclient-dev libmirclient9 libmircommon-dev libmircommon7\n",
            "  libmircookie-dev libmircookie2 libmircore-dev libmircore1 libmirprotobuf3\n",
            "  libpciaccess-dev libpixman-1-dev libprotobuf-dev libprotobuf-lite10\n",
            "  libpulse-dev libpulse-mainloop-glib0 libsndio-dev libudev-dev libxaw7-dev\n",
            "  libxcomposite-dev libxcursor-dev libxfont-dev libxinerama-dev\n",
            "  libxkbcommon-dev libxkbfile-dev libxmuu-dev libxpm-dev libxrandr-dev\n",
            "  libxres-dev libxres1 libxtst-dev libxv-dev libxvmc-dev libxvmc1\n",
            "  libxxf86dga-dev libxxf86dga1 mir-client-platform-mesa-dev swig3.0\n",
            "  x11proto-composite-dev x11proto-dri2-dev x11proto-fonts-dev x11proto-gl-dev\n",
            "  x11proto-randr-dev x11proto-record-dev x11proto-render-dev\n",
            "  x11proto-resource-dev x11proto-xf86bigfont-dev x11proto-xf86dga-dev\n",
            "  x11proto-xinerama-dev xserver-xorg-dev\n",
            "Suggested packages:\n",
            "  libxaw-doc libgle3 swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  gir1.2-ibus-1.0 libcapnp-0.6.1 libdbus-1-dev libdmx-dev libdmx1\n",
            "  libfontenc-dev libfs-dev libfs6 libibus-1.0-5 libibus-1.0-dev\n",
            "  libmirclient-dev libmirclient9 libmircommon-dev libmircommon7\n",
            "  libmircookie-dev libmircookie2 libmircore-dev libmircore1 libmirprotobuf3\n",
            "  libpciaccess-dev libpixman-1-dev libprotobuf-dev libprotobuf-lite10\n",
            "  libpulse-dev libpulse-mainloop-glib0 libsdl2-dev libsndio-dev libudev-dev\n",
            "  libxaw7-dev libxcomposite-dev libxcursor-dev libxfont-dev libxinerama-dev\n",
            "  libxkbcommon-dev libxkbfile-dev libxmuu-dev libxpm-dev libxrandr-dev\n",
            "  libxres-dev libxres1 libxtst-dev libxv-dev libxvmc-dev libxvmc1\n",
            "  libxxf86dga-dev libxxf86dga1 mir-client-platform-mesa-dev python-opengl swig\n",
            "  swig3.0 x11proto-composite-dev x11proto-dri2-dev x11proto-fonts-dev\n",
            "  x11proto-gl-dev x11proto-randr-dev x11proto-record-dev x11proto-render-dev\n",
            "  x11proto-resource-dev x11proto-xf86bigfont-dev x11proto-xf86dga-dev\n",
            "  x11proto-xinerama-dev xorg-dev xserver-xorg-dev xvfb\n",
            "0 upgraded, 64 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 7,598 kB of archives.\n",
            "After this operation, 44.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libdmx1 amd64 1:1.1.3-1 [10.4 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Err:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libibus-1.0-5 amd64 1.5.17-3ubuntu5\n",
            "  404  Not Found [IP: 91.189.88.24 80]\n",
            "Err:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gir1.2-ibus-1.0 amd64 1.5.17-3ubuntu5\n",
            "  404  Not Found [IP: 91.189.88.24 80]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcapnp-0.6.1 amd64 0.6.1-1ubuntu1 [658 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libdbus-1-dev amd64 1.12.2-1ubuntu1.1 [165 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libdmx-dev amd64 1:1.1.3-1 [34.4 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfontenc-dev amd64 1:1.1.3-1 [13.8 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfs6 amd64 2:1.0.7-1 [22.5 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-fonts-dev all 2018.4-4 [2,620 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfs-dev amd64 2:1.0.7-1 [26.8 kB]\n",
            "Err:12 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libibus-1.0-dev amd64 1.5.17-3ubuntu5\n",
            "  404  Not Found [IP: 91.189.88.24 80]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircore1 amd64 0.31.1-0ubuntu1 [26.5 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircommon7 amd64 0.31.1-0ubuntu1 [73.9 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libprotobuf-lite10 amd64 3.0.0-9.1ubuntu1 [97.7 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmirprotobuf3 amd64 0.31.1-0ubuntu1 [127 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmirclient9 amd64 0.31.1-0ubuntu1 [199 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircore-dev amd64 0.31.1-0ubuntu1 [21.7 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libprotobuf-dev amd64 3.0.0-9.1ubuntu1 [959 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxkbcommon-dev amd64 0.8.0-1ubuntu0.1 [308 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircommon-dev amd64 0.31.1-0ubuntu1 [13.9 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircookie2 amd64 0.31.1-0ubuntu1 [19.7 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircookie-dev amd64 0.31.1-0ubuntu1 [4,392 B]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmirclient-dev amd64 0.31.1-0ubuntu1 [47.8 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpixman-1-dev amd64 0.34.0-2 [244 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpulse-mainloop-glib0 amd64 1:11.1-1ubuntu7.2 [22.1 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpulse-dev amd64 1:11.1-1ubuntu7.2 [81.5 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsndio-dev amd64 1.1.0-3 [13.3 kB]\n",
            "Err:29 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libudev-dev amd64 237-3ubuntu10.25\n",
            "  404  Not Found [IP: 91.189.88.24 80]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxcursor-dev amd64 1:1.1.15-1 [26.5 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-xinerama-dev all 2018.4-4 [2,628 B]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxinerama-dev amd64 2:1.1.3-1 [8,404 B]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-randr-dev all 2018.4-4 [2,620 B]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxrandr-dev amd64 2:1.5.1-1 [24.0 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxv-dev amd64 2:1.0.11-1 [32.5 kB]\n",
            "Err:36 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsdl2-dev amd64 2.0.8+dfsg1-1ubuntu1.18.04.3\n",
            "  404  Not Found [IP: 91.189.88.24 80]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxpm-dev amd64 1:3.5.12-1 [87.4 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxaw7-dev amd64 2:1.0.13-1 [231 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-composite-dev all 1:2018.4-4 [2,620 B]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxcomposite-dev amd64 1:0.4.4-2 [9,136 B]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxfont-dev amd64 1:2.0.3-1 [118 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxkbfile-dev amd64 1:1.0.9-2 [74.3 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxmuu-dev amd64 2:1.1.2-2 [7,056 B]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxres1 amd64 2:1.2.0-2 [7,716 B]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-resource-dev all 2018.4-4 [2,620 B]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxres-dev amd64 2:1.2.0-2 [8,136 B]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-record-dev all 2018.4-4 [2,620 B]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxtst-dev amd64 2:1.2.3-1 [15.2 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxvmc1 amd64 2:1.0.10-1 [13.7 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxvmc-dev amd64 2:1.0.10-1 [21.3 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-xf86dga-dev all 2018.4-4 [2,624 B]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga-dev amd64 2:1.1.4-1 [17.6 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu bionic/main amd64 mir-client-platform-mesa-dev amd64 0.31.1-0ubuntu1 [11.0 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-dri2-dev all 2018.4-4 [2,620 B]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-gl-dev all 2018.4-4 [2,612 B]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-render-dev all 2:2018.4-4 [2,620 B]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-xf86bigfont-dev all 2018.4-4 [2,628 B]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpciaccess-dev amd64 0.14-1 [20.2 kB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 xserver-xorg-dev amd64 2:1.19.6-1ubuntu4.3 [198 kB]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 xorg-dev all 1:7.7+19ubuntu7.1 [4,300 B]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.3 [783 kB]\n",
            "Fetched 6,550 kB in 58s (113 kB/s)\n",
            "E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/i/ibus/libibus-1.0-5_1.5.17-3ubuntu5_amd64.deb  404  Not Found [IP: 91.189.88.24 80]\n",
            "E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/i/ibus/gir1.2-ibus-1.0_1.5.17-3ubuntu5_amd64.deb  404  Not Found [IP: 91.189.88.24 80]\n",
            "E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/i/ibus/libibus-1.0-dev_1.5.17-3ubuntu5_amd64.deb  404  Not Found [IP: 91.189.88.24 80]\n",
            "E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/s/systemd/libudev-dev_237-3ubuntu10.25_amd64.deb  404  Not Found [IP: 91.189.88.24 80]\n",
            "E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/universe/libs/libsdl2/libsdl2-dev_2.0.8+dfsg1-1ubuntu1.18.04.3_amd64.deb  404  Not Found [IP: 91.189.88.24 80]\n",
            "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n",
            "Collecting gym\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/88/a7186ffe1f33570ad3b8cd635996e5a3e3e155736e180ae6a2ad5e826a60/gym-0.15.3.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 2.8MB/s \n",
            "\u001b[?25hCollecting imageio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/de/f7f985018f462ceeffada7f6e609919fbcc934acd9301929cba14bc2c24a/imageio-2.6.1-py3-none-any.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 27.5MB/s \n",
            "\u001b[?25hCollecting PILLOW\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/66/6113477dc3206ccb1e192cffd626f2840ead02375a6cebe2436ad4c19f61/Pillow-6.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 26.8MB/s \n",
            "\u001b[?25hCollecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/cf/ad/b15f252bfb0f1693ad3150b55a44a674f3cba711cacdbb9ae2f03f143d19/PyVirtualDisplay-0.2.4-py2.py3-none-any.whl\n",
            "Collecting pyglet==1.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 16.3MB/s \n",
            "\u001b[?25hRequirement already up-to-date: pyopengl in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already up-to-date: scipy in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Collecting JSAnimation\n",
            "  Downloading https://files.pythonhosted.org/packages/3c/e6/a93a578400c38a43af8b4271334ed2444b42d65580f1d6721c9fe32e9fd8/JSAnimation-0.1.tar.gz\n",
            "Collecting opencv-python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/7e/bd5425f4dacb73367fddc71388a47c1ea570839197c2bcad86478e565186/opencv_python-4.1.1.26-cp36-cp36m-manylinux1_x86_64.whl (28.7MB)\n",
            "\u001b[K     |████████████████████████████████| 28.7MB 72.1MB/s \n",
            "\u001b[?25hCollecting h5py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/06/cafdd44889200e5438b897388f3075b52a8ef01f28a17366d91de0fa2d05/h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 27.4MB/s \n",
            "\u001b[?25hCollecting pyyaml\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/e8/b3212641ee2718d556df0f23f78de8303f068fe29cdaa7a91018849582fe/PyYAML-5.1.2.tar.gz (265kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 50.3MB/s \n",
            "\u001b[?25hCollecting hyperdash\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/a1/2606aa8a8c3bf083cb305dba3164cb00285f97db34080d63c22b6c413175/hyperdash-0.15.3.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.16.5)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Collecting cloudpickle~=1.2.0 (from gym)\n",
            "  Downloading https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\n",
            "Collecting EasyProcess (from pyvirtualdisplay)\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/29/40040d1d64a224a5e44df9572794a66494618ffe5c77199214aeceedb8a7/EasyProcess-0.2.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from pyglet==1.3.2) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from hyperdash) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: python-slugify in /usr/local/lib/python3.6/dist-packages (from hyperdash) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->hyperdash) (2019.9.11)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->hyperdash) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->hyperdash) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->hyperdash) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->hyperdash) (1.3)\n",
            "Building wheels for collected packages: gym, JSAnimation, pyyaml, hyperdash\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.15.3-cp36-none-any.whl size=1644971 sha256=cf17d4785b3f9f2757176a7056b647f666f255aee20c951c0345e6f746a56e6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/71/10/30f9b16332ecfd6318ac290445c696fe809bcbe40a05f9a799\n",
            "  Building wheel for JSAnimation (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for JSAnimation: filename=JSAnimation-0.1-cp36-none-any.whl size=11425 sha256=81aa4f1c2d1a571b4e0348ea03698f748d2bd4de469015281ee4adb9b5e5d082\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/c2/b2/b444dffc3eed9c78139288d301c4009a42c0dd061d3b62cead\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1.2-cp36-cp36m-linux_x86_64.whl size=44104 sha256=82bb3a26123818f7bad009150f3fd14c68756ef808fb6175bf28091955eb9713\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/45/dd/65f0b38450c47cf7e5312883deb97d065e030c5cca0a365030\n",
            "  Building wheel for hyperdash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hyperdash: filename=hyperdash-0.15.3-cp36-none-any.whl size=28553 sha256=bfdb0bafa9ceb50ad5f001c42caf24f1eee94bab423c559adc6812bb74dcc4af\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/5f/af/bbcaeb6570e4904c14fb4c1b70fee559a3788182ce4d104ce7\n",
            "Successfully built gym JSAnimation pyyaml hyperdash\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pyglet, cloudpickle, gym, PILLOW, imageio, EasyProcess, pyvirtualdisplay, JSAnimation, opencv-python, h5py, pyyaml, hyperdash\n",
            "  Found existing installation: pyglet 1.4.4\n",
            "    Uninstalling pyglet-1.4.4:\n",
            "      Successfully uninstalled pyglet-1.4.4\n",
            "  Found existing installation: cloudpickle 0.6.1\n",
            "    Uninstalling cloudpickle-0.6.1:\n",
            "      Successfully uninstalled cloudpickle-0.6.1\n",
            "  Found existing installation: gym 0.10.11\n",
            "    Uninstalling gym-0.10.11:\n",
            "      Successfully uninstalled gym-0.10.11\n",
            "  Found existing installation: Pillow 4.3.0\n",
            "    Uninstalling Pillow-4.3.0:\n",
            "      Successfully uninstalled Pillow-4.3.0\n",
            "  Found existing installation: imageio 2.4.1\n",
            "    Uninstalling imageio-2.4.1:\n",
            "      Successfully uninstalled imageio-2.4.1\n",
            "  Found existing installation: opencv-python 3.4.7.28\n",
            "    Uninstalling opencv-python-3.4.7.28:\n",
            "      Successfully uninstalled opencv-python-3.4.7.28\n",
            "  Found existing installation: h5py 2.8.0\n",
            "    Uninstalling h5py-2.8.0:\n",
            "      Successfully uninstalled h5py-2.8.0\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed EasyProcess-0.2.7 JSAnimation-0.1 PILLOW-6.2.0 cloudpickle-1.2.2 gym-0.15.3 h5py-2.10.0 hyperdash-0.15.3 imageio-2.6.1 opencv-python-4.1.1.26 pyglet-1.3.2 pyvirtualdisplay-0.2.4 pyyaml-5.1.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "cloudpickle"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 0 B/783 kB of archives.\n",
            "After this operation, 2,266 kB of additional disk space will be used.\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 131183 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.3_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.3) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.3) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFlP_AMJKDp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvKEUG38QgWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/drive/My\\ Drive/Colab\\ Notebooks/MT/Utils/xdpyinfo /usr/bin/\n",
        "!cp /content/drive/My\\ Drive/Colab\\ Notebooks/MT/Utils/libXxf86dga.* /usr/lib/x86_64-linux-gnu/\n",
        "!chmod +x /usr/bin/xdpyinfo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O2eiLR1ceiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!hyperdash signup --github"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPU0ipdJHy5S",
        "colab_type": "text"
      },
      "source": [
        "## Package Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02TShsX7QtEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "\n",
        "import gym\n",
        "from collections import deque\n",
        "from hyperdash import Experiment\n",
        "import cv2\n",
        "\n",
        "import pyvirtualdisplay\n",
        "import base64\n",
        "import IPython\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDl5L2b0RPOZ",
        "colab_type": "text"
      },
      "source": [
        "## Hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD9eJ4egQ3fg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Runtime settings\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n",
        "Transition = namedtuple('Transion', ('state', 'action', 'next_state', 'reward'))\n",
        "cv2.ocl.setUseOpenCL(False)\n",
        "time_stamp = str(int(time.time()))\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# Hyper parameters\n",
        "BATCH_SIZE = 32 # @param\n",
        "GAMMA = 0.99 # @param\n",
        "EPS_START = 1 # @param\n",
        "EPS_END = 0.02 # @param\n",
        "EPS_DECAY = 1000000 # @param\n",
        "TARGET_UPDATE = 1000 # @param\n",
        "DEFAULT_DURABILITY = 1000 # @param\n",
        "LEARNING_RATE = 1e-4 # @param\n",
        "INITIAL_MEMORY = 10000 # @param\n",
        "MEMORY_SIZE = 10 * INITIAL_MEMORY # @param\n",
        "DEFAULT_DURABILITY_DECREASED_LEVEL = 1 # @param\n",
        "DEFAULT_DURABILITY_INCREASED_LEVEL = 1 # @param\n",
        "DURABILITY_CHECK_FREQUENCY = 80 # @param\n",
        "\n",
        "# Some settings\n",
        "ENV_NAME = \"PongNoFrameskip-v4\" # @param\n",
        "EXP_NAME = \"PongNoFrameskip-v4_\" + time_stamp # @param\n",
        "RENDER = False # @param\n",
        "\n",
        "RUN_NAME = \"videos_proposal\" # @param\n",
        "output_directory = os.path.abspath(\n",
        "    os.path.join(os.path.curdir, \"/content/drive/My Drive/Colab Notebooks/MT/Runs\", ENV_NAME + \"_\" + RUN_NAME + \"_\" + time_stamp))\n",
        "\n",
        "TRAIN_LOG_FILE_PATH = output_directory + \"/\" + ENV_NAME + \"_train_\" + time_stamp + \".log\" # @param\n",
        "TEST_LOG_FILE_PATH = output_directory + \"/\" + ENV_NAME + \"_test_\" + time_stamp + \".log\" # @param\n",
        "PARAMETER_LOG_FILE_PATH = output_directory + \"/\" + ENV_NAME + \"_params_\" + time_stamp + \".json\" # @param\n",
        "\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "hyper_params = {\"BATCH_SIZE\": BATCH_SIZE, \"GAMMA\": GAMMA, \"EPS_START\": EPS_START,\n",
        "                \"EPS_END\": EPS_END, \"EPS_DECAY\": EPS_DECAY,\n",
        "                \"TARGET_UPDATE\": TARGET_UPDATE,\n",
        "                \"DEFAULT_DURABILITY\": DEFAULT_DURABILITY,\n",
        "                \"LEARNING_RATE\": LEARNING_RATE,\n",
        "                \"INITIAL_MEMORY\": INITIAL_MEMORY, \"MEMORY_SIZE\": MEMORY_SIZE,\n",
        "                \"DEFAULT_DURABILITY_DECREASED_LEVEL\": DEFAULT_DURABILITY_DECREASED_LEVEL,\n",
        "                \"DURABILITY_CHECK_FREQUENCY\": DURABILITY_CHECK_FREQUENCY,\n",
        "                \"ENV_NAME\" : ENV_NAME, \"EXP_NAME\": EXP_NAME, \n",
        "                \"TRAIN_LOG_FILE_PATH\": TRAIN_LOG_FILE_PATH,\n",
        "                \"TEST_LOG_FILE_PATH\": TEST_LOG_FILE_PATH,\n",
        "                \"PARAMETER_LOG_FILE_PATH\": PARAMETER_LOG_FILE_PATH,\n",
        "                \"RENDER\": str(RENDER)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y42MelxTn2q1",
        "colab_type": "text"
      },
      "source": [
        "## Define the Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzrZB3BZRUH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "        \n",
        "    def push(self, *args):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "        \n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "\n",
        "class PrioritizedReplay(object):\n",
        "    def __init__(self, capacity):\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLtkGPWnn8kJ",
        "colab_type": "text"
      },
      "source": [
        "## Define the DQNs\n",
        "\n",
        "Now we define the two types of DQN. One is simple q-network using 3 layers CNN. On the other one is batch normalaized 4 layers CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GlLM0L5R6HG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SQRT2 = math.sqrt(2.0)\n",
        "\n",
        "ACT = nn.ReLU\n",
        "\n",
        "\n",
        "class DQN(torch.jit.ScriptModule):\n",
        "    def __init__(self, in_channels=4, n_actions=14):\n",
        "        super(DQN, self).__init__()\n",
        "        self.convs = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4),\n",
        "            ACT(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            ACT(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            ACT()\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(7 * 7 * 64, 512),\n",
        "            ACT(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    @torch.jit.script_method\n",
        "    def forward(self, x):\n",
        "        x = x.float() / 255\n",
        "        x = self.convs(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "class DDQN(torch.jit.ScriptModule):\n",
        "    def __init__(self, in_channels=4, n_actions=14):\n",
        "        __constants__ = ['n_actions']\n",
        "\n",
        "        super(DDQN, self).__init__()\n",
        "\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "        self.convs = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4),\n",
        "            ACT(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            ACT(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            ACT()\n",
        "        )\n",
        "        self.fc_adv = nn.Sequential(\n",
        "            nn.Linear(7 * 7 * 64, 512),\n",
        "            ACT(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "        self.fc_val = nn.Sequential(\n",
        "            nn.Linear(7 * 7 * 64, 512),\n",
        "            ACT(),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "        def scale_grads_hook(module, grad_out, grad_in):\n",
        "            \"\"\"scale gradient by 1/sqrt(2) as in the original paper\"\"\"\n",
        "            grad_out = tuple(map(lambda g: g / SQRT2, grad_out))\n",
        "            return grad_out\n",
        "\n",
        "        self.fc_adv.register_backward_hook(scale_grads_hook)\n",
        "        self.fc_val.register_backward_hook(scale_grads_hook)\n",
        "\n",
        "    @torch.jit.script_method\n",
        "    def forward(self, x):\n",
        "        x = x.float() / 255\n",
        "        x = self.convs(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        adv = self.fc_adv(x)\n",
        "        val = self.fc_val(x)\n",
        "\n",
        "        return val + adv - adv.mean(1).unsqueeze(1)\n",
        "\n",
        "    @torch.jit.script_method\n",
        "    def value(self, x):\n",
        "        x = x.float() / 255\n",
        "        x = self.convs(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        return self.fc_val(x)\n",
        "\n",
        "\n",
        "class LanderDQN(torch.jit.ScriptModule):\n",
        "    def __init__(self, n_state, n_actions, nhid=64):\n",
        "        super(LanderDQN, self).__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(n_state, nhid),\n",
        "            ACT(),\n",
        "            nn.Linear(nhid, nhid),\n",
        "            ACT(),\n",
        "            nn.Linear(nhid, n_actions)\n",
        "        )\n",
        "\n",
        "    @torch.jit.script_method\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class RamDQN(torch.jit.ScriptModule):\n",
        "    def __init__(self, n_state, n_actions):\n",
        "        super(RamDQN, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(n_state, 256),\n",
        "            ACT(),\n",
        "            nn.Linear(256, 128),\n",
        "            ACT(),\n",
        "            nn.Linear(128, 64),\n",
        "            ACT(),\n",
        "            nn.Linear(64, n_actions)\n",
        "        )\n",
        "\n",
        "    @torch.jit.script_method\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class DQNbn(nn.Module):\n",
        "    def __init__(self, in_channels=4, n_actions=14):\n",
        "        \"\"\"\n",
        "        Initialize Deep Q Network\n",
        "        Args:\n",
        "            in_channels (int): number of input channels\n",
        "            n_actions (int): number of outputs\n",
        "        \"\"\"\n",
        "        super(DQNbn, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
        "        self.head = nn.Linear(512, n_actions)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.float() / 255\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
        "        return self.head(x)\n",
        "\n",
        "\n",
        "# class DQN(nn.Module):\n",
        "#     def __init__(self, in_channels=4, n_actions=14):\n",
        "#         \"\"\"\n",
        "#         Initialize Deep Q Network\n",
        "#         Args:\n",
        "#             in_channels (int): number of input channels\n",
        "#             n_actions (int): number of outputs\n",
        "#         \"\"\"\n",
        "#         super(DQN, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
        "#         # self.bn1 = nn.BatchNorm2d(32)\n",
        "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "#         # self.bn2 = nn.BatchNorm2d(64)\n",
        "#         self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "#         # self.bn3 = nn.BatchNorm2d(64)\n",
        "#         self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
        "#         self.head = nn.Linear(512, n_actions)\n",
        "        \n",
        "#     def forward(self, x):\n",
        "#         x = x.float() / 255\n",
        "#         x = F.relu(self.conv1(x))\n",
        "#         x = F.relu(self.conv2(x))\n",
        "#         x = F.relu(self.conv3(x))\n",
        "#         x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
        "#         return self.head(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FPJHZkWoCis",
        "colab_type": "text"
      },
      "source": [
        "## Define the Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g77AFM28oDRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, policy_net, target_net, durability, optimizer, name):\n",
        "        self.policy_net = policy_net\n",
        "        self.target_net = target_net\n",
        "        self.target_net.load_state_dict(policy_net.state_dict())\n",
        "        self.durability = durability\n",
        "        self.optimizer = optimizer\n",
        "        self.name = name\n",
        "        self.memory = ReplayMemory(MEMORY_SIZE)\n",
        "        self.steps_done = 0\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "\n",
        "    def select_action(self, state):\n",
        "        sample = random.random()\n",
        "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * self.steps_done / EPS_DECAY)\n",
        "        self.steps_done += 1\n",
        "        if sample > eps_threshold:\n",
        "            with torch.no_grad():\n",
        "              return self.policy_net(state.to('cuda')).max(1)[1].view(1,1)\n",
        "        else:\n",
        "            return torch.tensor([[random.randrange(4)]], device=device, dtype=torch.long)\n",
        "\n",
        "    \n",
        "    def optimize_model(self):\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "        transitions = self.memory.sample(BATCH_SIZE)\n",
        "        \"\"\"\n",
        "        zip(*transitions) unzips the transitions into\n",
        "        Transition(*) creates new named tuple\n",
        "        batch.state - tuple of all the states (each state is a tensor)\n",
        "        batch.next_state - tuple of all the next states (each state is a tensor)\n",
        "        batch.reward - tuple of all the rewards (each reward is a float)\n",
        "        batch.action - tuple of all the actions (each action is an int)    \n",
        "        \"\"\"\n",
        "        batch = Transition(*zip(*transitions))\n",
        "        \n",
        "        actions = tuple((map(lambda a: torch.tensor([[a]], device='cuda'), batch.action))) \n",
        "        rewards = tuple((map(lambda r: torch.tensor([r], device='cuda'), batch.reward))) \n",
        "\n",
        "        non_final_mask = torch.tensor(\n",
        "            tuple(map(lambda s: s is not None, batch.next_state)),\n",
        "            device=device, dtype=torch.bool)\n",
        "        \n",
        "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                        if s is not None]).to('cuda')\n",
        "        \n",
        "\n",
        "        state_batch = torch.cat(batch.state).to('cuda')\n",
        "        action_batch = torch.cat(actions)\n",
        "        reward_batch = torch.cat(rewards)\n",
        "        \n",
        "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
        "        \n",
        "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
        "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "        \n",
        "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        for param in self.policy_net.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.state\n",
        "\n",
        "      \n",
        "    def set_state(self, state):\n",
        "        self.state = state\n",
        "        \n",
        "\n",
        "    def set_env(self, env):\n",
        "        self.env = env\n",
        "\n",
        "\n",
        "    def get_env(self):\n",
        "        return self.env\n",
        "\n",
        "\n",
        "    def set_action(self, action):\n",
        "        self.action = action\n",
        "\n",
        "\n",
        "    def get_action(self):\n",
        "        return self.action\n",
        "\n",
        "\n",
        "    def get_durability(self):\n",
        "        return self.durability\n",
        "\n",
        "\n",
        "    def get_policy_net(self):\n",
        "        return self.policy_net\n",
        "\n",
        "    \n",
        "    def reduce_durability(self, value):\n",
        "        self.durability = self.durability - value\n",
        "\n",
        "    def heal_durability(self, value):\n",
        "        self.durability = self.durability + value\n",
        "    \n",
        "    def set_done_state(self, done):\n",
        "        self.done = done\n",
        "    \n",
        "\n",
        "    def set_total_reward(self, reward):\n",
        "        self.reward = self.reward + reward\n",
        "    \n",
        "    def get_total_reward(self):\n",
        "        return self.total_reward\n",
        "\n",
        "\n",
        "    def set_step_retrun_value(self, obs, reward, done, info):\n",
        "        self.obs = obs\n",
        "        self.reward = reward\n",
        "        self.done = done\n",
        "        self.info = info\n",
        "        \n",
        "\n",
        "    def is_done(self):\n",
        "        return self.done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLx6x64KoE9Y",
        "colab_type": "text"
      },
      "source": [
        "## Define the Environment\n",
        "\n",
        "**TODO: Make sure to create environment class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF7Ve2SdR6dL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def make_env(env, stack_frames=True, episodic_life=True, clip_rewards=False, scale=False):\n",
        "#     if episodic_life:\n",
        "#         env = EpisodicLifeEnv(env)\n",
        "#\n",
        "#     env = NoopResetEnv(env, noop_max=30)\n",
        "#     env = MaxAndSkipEnv(env, skip=4)\n",
        "#     if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "#         env = FireResetEnv(env)\n",
        "#\n",
        "#     env = WarpFrame(env)\n",
        "#     if stack_frames:\n",
        "#         env = FrameStack(env, 4)\n",
        "#     if clip_rewards:\n",
        "#         env = ClipRewardEnv(env)\n",
        "#     return env\n",
        "\n",
        "\n",
        "def get_state(obs):\n",
        "    state = np.array(obs)\n",
        "    state = state.transpose((2, 0, 1))\n",
        "    state = torch.from_numpy(state)\n",
        "    return state.unsqueeze(0)\n",
        "\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        self.env = self.make_env(self.env)\n",
        "        \n",
        "    def get_env(self):\n",
        "        return self.env\n",
        "\n",
        "\n",
        "    def make_env(self, env, stack_frames=True, episodic_life=True, clip_rewards=False, scale=False):\n",
        "        if episodic_life:\n",
        "            env = EpisodicLifeEnv(env)\n",
        "\n",
        "        env = NoopResetEnv(self.env, noop_max=30)\n",
        "        env = MaxAndSkipEnv(self.env, skip=4)\n",
        "        if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "            env = FireResetEnv(self.env)\n",
        "\n",
        "        env = WarpFrame(env)\n",
        "        if stack_frames:\n",
        "            env = FrameStack(env, 4)\n",
        "        if clip_rewards:\n",
        "            env = ClipRewardEnv(env)\n",
        "        return env\n",
        "\n",
        "\n",
        "    def get_state(obs):\n",
        "        state = np.array(obs)\n",
        "        state = state.transpose((2, 0, 1))\n",
        "        state = torch.from_numpy(state)\n",
        "        return state.unsqueeze(0)\n",
        "\n",
        "    \n",
        "class RewardScaler(gym.RewardWrapper):\n",
        "\n",
        "    def reward(self, reward):\n",
        "        return reward * 0.1\n",
        "\n",
        "\n",
        "class ClipRewardEnv(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.RewardWrapper.__init__(self, env)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
        "        return np.sign(reward)\n",
        "\n",
        "\n",
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\n",
        "        This object should only be converted to numpy array before being passed to the model.\n",
        "        You'd not believe how complex the previous solution was.\"\"\"\n",
        "        self._frames = frames\n",
        "        self._out = None\n",
        "\n",
        "    def _force(self):\n",
        "        if self._out is None:\n",
        "            self._out = np.concatenate(self._frames, axis=2)\n",
        "            self._frames = None\n",
        "        return self._out\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = self._force()\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._force())\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self._force()[i]\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "        See Also\n",
        "        --------\n",
        "        baselines.common.atari_wrappers.LazyFrames\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * k), dtype=env.observation_space.dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        ob = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob()\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, done, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n",
        "\n",
        "\n",
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.width = 84\n",
        "        self.height = 84\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
        "            shape=(self.height, self.width, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, frame):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "        return frame[:, :, None]\n",
        "\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        \"\"\"For environments where the user need to press FIRE for the game to start.\"\"\"\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        return obs\n",
        "\n",
        "\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        super(EpisodicLifeEnv, self).__init__(env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done = True\n",
        "        self.was_real_reset = False\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert somtimes we stay in lives == 0 condtion for a few frames\n",
        "            # so its important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset()\n",
        "            self.was_real_reset = True\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "            self.was_real_reset = False\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs\n",
        "\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n",
        "        self._obs_buffer.clear()\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        super(NoopResetEnv, self).__init__(env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset()\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            noops = np.random.randint(1, self.noop_max + 1)\n",
        "        assert noops > 0\n",
        "        obs = None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _ = self.env.step(0)\n",
        "            if done:\n",
        "                obs = self.env.reset()\n",
        "        return obs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv-Dqojqm7Nh",
        "colab_type": "text"
      },
      "source": [
        "## Deprecated code\n",
        "\n",
        "**Thease code move to agent class.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTc_yZ_YSNqk",
        "colab_type": "text"
      },
      "source": [
        "@deprecated\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END)* \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            return policy_net(state.to('cuda')).max(1)[1].view(1,1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(4)]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "@deprecated\n",
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    \"\"\"\n",
        "    zip(*transitions) unzips the transitions into\n",
        "    Transition(*) creates new named tuple\n",
        "    batch.state - tuple of all the states (each state is a tensor)\n",
        "    batch.next_state - tuple of all the next states (each state is a tensor)\n",
        "    batch.reward - tuple of all the rewards (each reward is a float)\n",
        "    batch.action - tuple of all the actions (each action is an int)    \n",
        "    \"\"\"\n",
        "    batch = Transition(*zip(*transitions))\n",
        "    \n",
        "    actions = tuple((map(lambda a: torch.tensor([[a]], device='cuda'), batch.action))) \n",
        "    rewards = tuple((map(lambda r: torch.tensor([r], device='cuda'), batch.reward))) \n",
        "\n",
        "    non_final_mask = torch.tensor(\n",
        "        tuple(map(lambda s: s is not None, batch.next_state)),\n",
        "        device=device, dtype=torch.uint8)\n",
        "    \n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                       if s is not None]).to('cuda')\n",
        "    \n",
        "\n",
        "    state_batch = torch.cat(batch.state).to('cuda')\n",
        "    action_batch = torch.cat(actions)\n",
        "    reward_batch = torch.cat(rewards)\n",
        "    \n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "    \n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "    \n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTymzujboNgi",
        "colab_type": "text"
      },
      "source": [
        "## Degine the train steps\n",
        "\n",
        "In my research, make this code multi-agent (**Note**: Multi-agent here means multiple independent agents sharing a task environment)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0rY86vwSU3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO : To change the deprecated function to Agent clsss fuction\n",
        "def train(envs, agents, core_env, core_agent, n_episodes, agent_n, exp, render=False):\n",
        "    \"\"\"\n",
        "    Training step.\n",
        "\n",
        "    In this code, we use the multi-agents to create candidate for core agent.\n",
        "    The core agent and environment is main RL set. In addition, each agent has\n",
        "    own environment and durabiliry. Each agent's reward is checked for the\n",
        "    specified number of episodes, and if an agent is not selected as the\n",
        "    best-agent, that agent's durability is reduced.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    envs: list of Environment\n",
        "        List of environment for multi-agent\n",
        "    agents: list of Agent\n",
        "        List of multi-agents to create candidates for core_agent\n",
        "    core_env: Environment\n",
        "        Main environment of this train step\n",
        "    core_agent: Agent\n",
        "        Main agent of this train step\n",
        "    n_episodes: int\n",
        "        The number of episodes\n",
        "    agent_n : int\n",
        "        The number of agent\n",
        "    exp: Experiment\n",
        "        The Experiment object used by hyperdash\n",
        "    render: boolean, default False\n",
        "        Flag for whether to render the environment\n",
        "    \"\"\"\n",
        "    for episode in range(n_episodes):\n",
        "        print(\"episode: {}\".format(episode));\n",
        "        # 0. Initalize the environment, state and agent params\n",
        "        obs = core_env.reset()\n",
        "        core_state = get_state(obs)\n",
        "        core_agent.total_reward = 0.0\n",
        "        core_agent.set_state(core_state)\n",
        "        for agent in agents:\n",
        "            obs = agent.get_env().reset()\n",
        "            state = get_state(obs)\n",
        "            agent.set_state(state)\n",
        "            agent.total_reward = 0.0\n",
        "            # agent.durability = DEFAULT_DURABILITY\n",
        "\n",
        "\n",
        "        for t in count():\n",
        "            #if t % 20 != 0:\n",
        "            #    print(str(t) + \" \", end='')\n",
        "            #else:\n",
        "            #    print(\"\\n\")\n",
        "            #    print([agent.get_durability() for agent in agents])\n",
        "            #    print(str(t) + \" \", end='')            \"\"\"\"\n",
        "            \n",
        "            # 1. Select action from environment of each agent\n",
        "            for agent in agents:\n",
        "                agent.set_env(core_agent.get_env())\n",
        "                action = agent.select_action(agent.get_state())\n",
        "                agent.set_action(action)\n",
        "            \n",
        "            # 2. Proceed step of each agent\n",
        "            for agent in agents:\n",
        "                obs, reward, done, info = agent.get_env().step(agent.get_action())\n",
        "                agent.set_step_retrun_value(obs, reward, done, info)\n",
        "\n",
        "                agent.set_total_reward(reward)\n",
        "\n",
        "                if not done:\n",
        "                    next_state = get_state(obs)\n",
        "                else:\n",
        "                    next_state = None\n",
        "\n",
        "                reward = torch.tensor([reward], device=device)\n",
        "\n",
        "                agent.memory.push(agent.get_state(), agent.get_action().to('cpu'),\n",
        "                                  next_state, reward.to('cpu'))\n",
        "                agent.set_state(next_state)\n",
        "\n",
        "                if agent.steps_done > INITIAL_MEMORY:\n",
        "                    agent.optimize_model()\n",
        "\n",
        "                    if agent.steps_done % TARGET_UPDATE == 0:\n",
        "                        agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
        "            \n",
        "            # ---------------\n",
        "            # Proposal method\n",
        "            # ---------------\n",
        "            \n",
        "            # 3. Select best agent in this step\n",
        "            reward_list = [agent.get_total_reward() for agent in agents]\n",
        "            best_agents = [i for i, v in enumerate(reward_list) if v == max(reward_list)]\n",
        "            best_agent_index = random.choice(best_agents)\n",
        "            best_agent = agents[best_agent_index]\n",
        "            best_agent.heal_durability(DEFAULT_DURABILITY_INCREASED_LEVEL)\n",
        "            \n",
        "            \n",
        "            # Best_agent infomation\n",
        "            # exp.log(\"Current best agent: {}\".format(best_agent.name))\n",
        "\n",
        "            # 4. Check the agent durability in specified step\n",
        "            if t % DURABILITY_CHECK_FREQUENCY == 0:\n",
        "                if len(agents) > 1:\n",
        "                    index = [i for i in range(len(agents)) if i not in best_agents]\n",
        "                    for i in index:\n",
        "                        agents[i].reduce_durability(DEFAULT_DURABILITY_DECREASED_LEVEL)\n",
        "\n",
        "            # 5. Main step of core agent\n",
        "            core_agent_action = best_agent.get_action()\n",
        "            core_agent.set_action(core_agent_action)\n",
        "\n",
        "            core_obs, core_reward, core_done, core_info = core_agent.get_env().step(\n",
        "                core_agent.get_action())\n",
        "            core_agent.set_step_retrun_value(core_obs, core_reward, core_done, core_info)\n",
        "\n",
        "            core_agent.set_done_state(core_done)\n",
        "            core_agent.set_total_reward(core_reward)\n",
        "\n",
        "            if not core_done:\n",
        "                core_next_state = get_state(core_obs)\n",
        "            else:\n",
        "                core_next_state = None\n",
        "\n",
        "            core_reward = torch.tensor([core_reward], device=device)\n",
        "\n",
        "            core_agent.memory.push(core_agent.get_state(),\n",
        "                                   core_agent.get_action().to('cpu'),\n",
        "                                   core_next_state, core_reward.to('cpu'))\n",
        "            core_agent.set_state(core_next_state)\n",
        "\n",
        "            if core_agent.steps_done > INITIAL_MEMORY:\n",
        "                core_agent.optimize_model()\n",
        "\n",
        "                if core_agent.steps_done % TARGET_UPDATE == 0:\n",
        "                    core_agent.target_net.load_state_dict(core_agent.policy_net.state_dict())\n",
        "\n",
        "            if core_agent.is_done():\n",
        "                print(\"\\n\")\n",
        "                break\n",
        "        \n",
        "        # 6. Swap agent\n",
        "        if len(agents) > 1 and episode % DURABILITY_CHECK_FREQUENCY == 0:\n",
        "            for agent, i in zip(agents, range(len(agents))):\n",
        "                if agent.durability <= 0:\n",
        "                    del agents[i]\n",
        "\n",
        "        # ----------------------\n",
        "        # End of proposal method\n",
        "        # ----------------------\n",
        "        \n",
        "        exp.metric(\"total_reward\", core_agent.get_total_reward())\n",
        "        out_str = 'Total steps: {} \\t Episode: {}/{} \\t Total reward: {}'.format(\n",
        "            core_agent.steps_done, episode, t, core_agent.total_reward)\n",
        "        if episode % 20 == 0:\n",
        "            print(out_str)\n",
        "            out_str = str(\"\\n\" + out_str + \"\\n\")\n",
        "            exp.log(out_str)\n",
        "        else:\n",
        "            print(out_str)\n",
        "            exp.log(out_str)     \n",
        "        #with open(TRAIN_LOG_FILE_PATH, 'wt') as f:\n",
        "        #     f.write(out_str)\n",
        "    env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r34-vdKoRXs",
        "colab_type": "text"
      },
      "source": [
        "## Define the test steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUzw2mQ3T--m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO : To change the deprecated function to Agent clsss fuction\n",
        "def test(env, n_episodes, policy, exp, render=True):\n",
        "    # Save video as mp4 on specified directory\n",
        "    env = gym.wrappers.Monitor(env, './videos/' + 'dqn_pong_video')\n",
        "    for episode in range(n_episodes):\n",
        "        obs = env.reset()\n",
        "        state = env.get_state(obs)\n",
        "        total_reward = 0.0\n",
        "        for t in count():\n",
        "            action = policy(state.to('cuda')).max(1)[1].view(1,1)\n",
        "\n",
        "            if render:\n",
        "                env.render()\n",
        "                time.sleep(0.02)\n",
        "\n",
        "            obs, reward, done, info = env.step(action)\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            if not done:\n",
        "                next_state = env.get_state(obs)\n",
        "            else:\n",
        "                next_state = None\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                out_str = \"Finished Episode {} with reward {}\".format(\n",
        "                    episode, total_reward)\n",
        "                print(out_str)\n",
        "                exp.log(out_str)\n",
        "                with open(TEST_LOG_FILE_NAME, 'wt') as f:\n",
        "                    f.write(out_str)\n",
        "                break\n",
        "\n",
        "    env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y89M-d5CoSgs",
        "colab_type": "text"
      },
      "source": [
        "## Main steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P3NprG-6BBgW",
        "colab": {}
      },
      "source": [
        "# Create Agent\n",
        "agents = []\n",
        "\n",
        "policy_net_0 = DQN(n_actions=4).to(device)\n",
        "target_net_0 = DQN(n_actions=4).to(device)\n",
        "optimizer_0 = optim.Adam(policy_net_0.parameters(), lr=LEARNING_RATE)\n",
        "agents.append(Agent(policy_net_0, target_net_0, DEFAULT_DURABILITY,\n",
        "                    optimizer_0, \"cnn-dqn0\"))\n",
        "agents.append(Agent(policy_net_0, target_net_0, DEFAULT_DURABILITY,\n",
        "                    optimizer_0, \"cnn-dqn1\"))\n",
        "policy_net_1 = DDQN(n_actions=4).to(device)\n",
        "target_net_1 = DDQN(n_actions=4).to(device)\n",
        "optimizer_1 = optim.Adam(policy_net_1.parameters(), lr=LEARNING_RATE)\n",
        "agents.append(Agent(policy_net_1, target_net_1, DEFAULT_DURABILITY,\n",
        "                    optimizer_1, \"cnn-ddqn0\"))\n",
        "agents.append(Agent(policy_net_1, target_net_1, DEFAULT_DURABILITY,\n",
        "                    optimizer_1, \"cnn-ddqn1\"))\n",
        "\n",
        "core_agent = Agent(policy_net_0, target_net_0, DEFAULT_DURABILITY, optimizer_0,\n",
        "                   \"core\")\n",
        "\n",
        "AGENT_N = len(agents)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3E3SgmPxxDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# time_stamp = str(int(time.time()))\n",
        "hyper_params[\"AGENT_N\"] = AGENT_N\n",
        "json_params = json.dumps(hyper_params)\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "with open(PARAMETER_LOG_FILE_PATH, 'wt') as f:\n",
        "    f.write(json_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_X0a09mUKtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Deprecated code\n",
        "# create networks\n",
        "# policy_net = DQN(n_actions=4).to(device)\n",
        "# target_net = DQN(n_actions=4).to(device)\n",
        "# target_net.load_state_dict(policy_net.state_dict())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaUBflT4zvZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create environment\n",
        "# TODO: Create Environment class\n",
        "\n",
        "# env = gym.make(ENV_NAME)\n",
        "# env = make_env(env)\n",
        "envs = []\n",
        "for i in range(AGENT_N):\n",
        "    env = Environment()\n",
        "    env = env.get_env()\n",
        "    envs.append(env)\n",
        "\n",
        "core_env = Environment()\n",
        "core_env = core_env.get_env()\n",
        "\n",
        "for agent, env in zip(agents, envs):\n",
        "    agent.set_env(env)\n",
        "core_agent.set_env(core_env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvzpJPerUSIe",
        "colab_type": "code",
        "outputId": "5a104bdf-d799-4851-9fc7-695b3d279156",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# setup optimizer\n",
        "# optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# steps_done = 0\n",
        "\n",
        "# Deprecated\n",
        "# initialize replay memory\n",
        "# memory = ReplayMemory(MEMORY_SIZE)\n",
        "\n",
        "# Hyperdash experiment\n",
        "exp = Experiment(EXP_NAME, capture_io=False)\n",
        "print(\"Learning rate:{}\".format(LEARNING_RATE))\n",
        "exp.param(\"Learning rate\", LEARNING_RATE)\n",
        "exp.param(\"Environment\", ENV_NAME)\n",
        "exp.param(\"Batch size\", BATCH_SIZE)\n",
        "exp.param(\"Gamma\", GAMMA)\n",
        "exp.param(\"Episode start\", EPS_START)\n",
        "exp.param(\"Episode end\", EPS_END)\n",
        "exp.param(\"Episode decay\", EPS_DECAY)\n",
        "exp.param(\"Target update\", TARGET_UPDATE)\n",
        "exp.param(\"Render\", str(RENDER))\n",
        "exp.param(\"Initial memory\", INITIAL_MEMORY)\n",
        "exp.param(\"Memory size\", MEMORY_SIZE)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate:0.0001\n",
            "{ Learning rate: 0.0001 }\n",
            "{ Environment: PongNoFrameskip-v4 }\n",
            "{ Batch size: 32 }\n",
            "{ Gamma: 0.99 }\n",
            "{ Episode start: 1 }\n",
            "{ Episode end: 0.02 }\n",
            "{ Episode decay: 1000000 }\n",
            "{ Target update: 1000 }\n",
            "{ Render: False }\n",
            "{ Initial memory: 10000 }\n",
            "{ Memory size: 100000 }\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RD3exzskzIiY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train model\n",
        "train(envs, agents, core_env, core_agent, 400, AGENT_N, exp)\n",
        "exp.end()\n",
        "torch.save(policy_net, output_directory + \"/dqn_pong_model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9XQKShT4C2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EB\n",
        "exp.end()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY3kuj5wUUTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test model\n",
        "test_env = Environment()\n",
        "test_env = env.get_env()\n",
        "\n",
        "policy_net = torch.load(output_directory + \"/dqn_pong_model\")\n",
        "exp_test = Experiment(str(EXP_NAME + \"_test_step\"), capture_io=False)\n",
        "test(test_env, 1, policy_net, exp_test, render=False)\n",
        "exp_test.end()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X4JjgzwoYwk",
        "colab_type": "text"
      },
      "source": [
        "## Video vidualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BSXD_DlKwl70",
        "colab": {}
      },
      "source": [
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
        "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ewt5ZVWUgIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embed_mp4(filename):\n",
        "    \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "    \n",
        "    video = open(filename,'rb').read()\n",
        "    b64 = base64.b64encode(video)\n",
        "    tag = '''\n",
        "    <video width=\"640\" height=\"480\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "    Your browser does not support the video tag.\n",
        "    </video>'''.format(b64.decode())\n",
        "\n",
        "    return IPython.display.HTML(tag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Uem40STcclw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_mp4(\"/content/videos/dqn_pong_video/openaigym.video.0.122.video000000.mp4\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ONlurrP13Xgu",
        "colab": {}
      },
      "source": [
        "# !mv /content/drive/My\\ Drive/Colab\\ Notebooks/MT/pong_videos /content/drive/My\\ Drive/Colab\\ Notebooks/MT/pong_videos_1567682751\n",
        "# !mv /content/dqn_pong_model /content/drive/My\\ Drive/Colab\\ Notebooks/MT/pong_videos_1567682751/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkMzJ4wU11Rl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/drive/My\\ Drive/Colab\\ Notebooks/MT/pong_videos_1568005544"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvsieYOEBfP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv ./PongNoFrameskip-v4_*.log /content/drive/My\\ Drive/Colab\\ Notebooks/MT/pong_videos_1568005544/\n",
        "!mv ./dqn_pong_model /content/drive/My\\ Drive/Colab\\ Notebooks/MT/pong_videos_1568005544/\n",
        "!mv ./videos /content/drive/My\\ Drive/Colab\\ Notebooks/MT/pong_videos_1568005544/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}